{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfflEIXVBouWYs10p48wlM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lewale1/Testweb/blob/master/IS421_Final_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KahmCnNt6_nM"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        " install.packages('tidymodels')\n",
        " install.packages ('tidyverse')\n",
        " install.packages ('vip')\n",
        "\n",
        "  uninstall.packages ('ggplot2')\n",
        "  install.packages('ggplot2')\n",
        "\n",
        "# Load the tidymodels library\n",
        "\n",
        "\n",
        "#clear all R objects from memory\n",
        "rm(list = ls())\n",
        "\n",
        "#In this Lab, we will learn about linear regression with tidymodels. We will start by\n",
        "#fitting a linear regression model to the advertising data set used in the textbook,\n",
        "#\"An Introduction to Statistical Learning\".\n",
        "\n",
        "#Then we will focus on building our first machine learning pipeline, with data resampling,\n",
        "#featuring engineering, modeling fitting, and model accuracy assessment using the\n",
        "#workflows, rsample, recipes, parsnip, and tune packages from tidymodels.\n",
        "\n",
        "# Load libraries\n",
        "library(tidyverse)\n",
        "library(tidymodels)\n",
        "library(vip) # for variable importance\n",
        "#The vip package is used for exploring predictor variable importance. We will use this\n",
        "#package for visualizing which predictors have the most predictive power in our linear\n",
        "#regression models.\n",
        "\n",
        "# Load data sets\n",
        "url <- 'https://gmudatamining.com/data/advertising.rds'\n",
        "advertising <- read_rds(url)\n",
        "advertising\n",
        "\n",
        "home_sales <- read_rds('https://gmudatamining.com/data/home_sales.rds') %>%\n",
        "   select(-selling_date)\n",
        "home_sales\n",
        "\n",
        "#Data\n",
        "#We will be working with the advertising data set, where each row represents a store\n",
        "#from a large retail chain and their associated sales revenue and advertising budgets,\n",
        "#and the home_sales data, where each row represents a real estate home sale in the\n",
        "#Seattle area between 2014 and 2015. Take a moment to have a look these two data sets.\n",
        "\n",
        "#Data Splitting\n",
        "#The first step in building regression models is to split our original data into a\n",
        "#training and test set. We then perform all feature engineering and model fitting tasks\n",
        "#on the training set and use the test set as an independent assessment of our model's\n",
        "#prediction accuracy.\n",
        "\n",
        "#We will be using the initial_split() function from rsample to partition the advertising\n",
        "#data into training and test sets. Remember to always use set.seed() to ensure your\n",
        "#results are reproducible.\n",
        "set.seed(314)\n",
        "\n",
        "# Create a split object\n",
        "advertising_split <- initial_split(advertising, prop = 0.75, strata = Sales)\n",
        "\n",
        "# Build training data set\n",
        "advertising_training <- advertising_split %>%\n",
        "   training()\n",
        "\n",
        "# Build testing data set\n",
        "advertising_test <- advertising_split %>%\n",
        "   testing()\n",
        "\n",
        "#-----Model Specification-----#\n",
        "#The next step in the process is to build a linear regression model object to which we\n",
        "#fit our training data. For every model type, such as linear regression, there are\n",
        "#numerous packages (or engines) in R that can be used.\n",
        "\n",
        "#For example, we can use the lm() function from base R or the stan_glm() function from\n",
        "#the rstanarm package. Both of these functions will fit a linear regression model to our\n",
        "#data with slightly different implementations.\n",
        "\n",
        "#The parsnip package from tidymodels acts like an aggregator across the various modeling\n",
        "#engines within R. This makes it easy to implement machine learning algorithms from\n",
        "#different R packages with one unifying syntax.\n",
        "\n",
        "#To specify a model object with parsnip, we must:\n",
        "#1. Pick a model type\n",
        "#2. Set the engine\n",
        "#3. Set the mode (either regression or classification)\n",
        "\n",
        "#Linear regression is implemented with the linear_reg() function in parsnip. To\n",
        "#set the engine and mode, we use set_engine() and set_mode() respectively. Each one of\n",
        "#these functions takes a parsnip object as an argument and updates its properties.\n",
        "\n",
        "#To explore all parsnip models, please see the documentation where you can search by\n",
        "#keyword. Let's create a linear regression model object with the lm engine. This is the\n",
        "#default engine for most applications.\n",
        "lm_model <- linear_reg() %>%\n",
        "   set_engine('lm') %>% # adds lm implementation of linear regression\n",
        "   set_mode('regression')\n",
        "\n",
        "# View object properties\n",
        "lm_model\n",
        "\n",
        "#Fitting to Training Data\n",
        "#Now we are ready to train our model object on the advertising_training data. We can\n",
        "#do this using the fit() function from the parsnip package. The fit() function takes\n",
        "#the following arguments:\n",
        "\n",
        "#a parnsip model object specification\n",
        "#a model formula\n",
        "#a data frame with the training data\n",
        "\n",
        "#The code below trains our linear regression model on the advertising_training data.\n",
        "#In our formula, we have specified that Sales is the response variable and TV, Radio,\n",
        "#and Newspaper are our predictor variables.\n",
        "#We have assigned the name lm_fit to our trained linear regression model.\n",
        "lm_fit <- lm_model %>%\n",
        "   fit(Sales ~ ., data = advertising_training)\n",
        "\n",
        "# View lm_fit properties\n",
        "lm_fit\n",
        "\n",
        "#Exploring Training Results\n",
        "#As mentioned in the first R tutorial, most model objects in R are stored as specialized\n",
        "#lists. The lm_fit object is list that contains all of the information about how our\n",
        "#model was trained as well as the detailed results. Let's use the names() function to\n",
        "#print the named objects that are stored within lm_fit. The important objects are fit\n",
        "#and preproc. These contain the trained model and preprocessing steps (if any are used),\n",
        "#respectively.\n",
        "names(lm_fit)\n",
        "\n",
        "#To print a summary of our model, we can extract fit from lm_fit and pass it to the\n",
        "#summary() function. We can explore the estimated coefficients, F-statistics, p-values,\n",
        "#residual standard error (also known as RMSE) and R2 value.\n",
        "\n",
        "#However, this feature is best for visually exploring our results on the training data\n",
        "#since the results are returned as a data frame.\n",
        "summary(lm_fit$fit)\n",
        "\n",
        "#We can use the plot() function to obtain diagnostic plots for our trained regression\n",
        "#model. Again, we must first extract the fit object from lm_fit and then pass it into\n",
        "#plot(). These plots provide a check for the main assumptions of the linear regression\n",
        "#model.\n",
        "par(mfrow=c(1,1)) # plot all 4 plots in one\n",
        "\n",
        "plot(lm_fit$fit,\n",
        "     pch = 16,    # optional parameters to make points blue\n",
        "     col = '#006EA1')\n",
        "\n",
        "#Tidy Training Results\n",
        "#To obtain the detailed results from our trained linear regression model in a data\n",
        "#frame, we can use the tidy() and glance() functions directly on our trained parsnip\n",
        "#model, lm_fit.\n",
        "#The tidy() function takes a linear regression object and returns a data frame of the\n",
        "#estimated model coefficients and their associated F-statistics and p-values.\n",
        "#The glance() function will return performance metrics obtained on the training data\n",
        "#such as the R2 value (r.squared) and the RMSE (sigma).\n",
        "\n",
        "# Data frame of estimated coefficients\n",
        "tidy(lm_fit)\n",
        "\n",
        "# Performance metrics on training data\n",
        "glance(lm_fit)\n",
        "\n",
        "#We can also use the vip() function to plot the variable importance for each predictor\n",
        "#in our model. The importance value is determined based on the F-statistics and estimate\n",
        "#coefficents in our trained model object.\n",
        "vip(lm_fit)\n",
        "\n",
        "#Evaluating Test Set Accuracy\n",
        "#To assess the accuracy of our trained linear regression model, lm_fit, we must use it\n",
        "#to make predictions on our test data, advertising_test.\n",
        "#This is done with the predict() function from parnsip. This function takes two important\n",
        "#arguments:\n",
        "# -a trained parnsip model object\n",
        "# -new_data for which to generate predictions\n",
        "\n",
        "#The code below uses the predict function to generate a data frame with a single column,\n",
        "#.pred, which contains the predicted Sales values on the advertisting_test data.\n",
        "predict(lm_fit, new_data = advertising_test)\n",
        "\n",
        "#Generally it's best to combine the test data set and the predictions into a single\n",
        "#data frame. We create a data frame with the predictions on the advertising_test data\n",
        "#and then use bind_cols to add the advertising_test data to the results.\n",
        "\n",
        "#Now we have the model results and the test data in a single data frame.\n",
        "advertising_test_results <- predict(lm_fit, new_data = advertising_test) %>%\n",
        "   bind_cols(advertising_test)\n",
        "\n",
        "# View results\n",
        "advertising_test_results\n",
        "\n",
        "#Calculating RMSE and R2 on the Test Data\n",
        "#To obtain the RMSE and R2 values on our test set results, we can use the rmse() and\n",
        "#rsq() functions. Both functions take the following arguments:\n",
        "# -data - a data frame with columns that have the true values and predictions\n",
        "# -truth - the column with the true response values\n",
        "# -estimate - the column with predicted values\n",
        "\n",
        "#In the examples below we pass our advertising_test_results to these functions to\n",
        "#obtain these values for our test set. results are always returned as a data frame with\n",
        "#the following columns: .metric, .estimator, and .estimate.\n",
        "\n",
        "# RMSE on test set\n",
        "rmse(advertising_test_results,\n",
        "     truth = Sales,\n",
        "     estimate = .pred)\n",
        "\n",
        "# R2 on test set\n",
        "rsq(advertising_test_results,\n",
        "    truth = Sales,\n",
        "    estimate = .pred)\n",
        "\n",
        "#R2 Plot\n",
        "#The best way to assess the test set accuracy is by making an R2 plot. This is a plot\n",
        "#that can be used for any regression model.\n",
        "#It plots the actual values (Sales) versus the model predictions (.pred) as a scatter\n",
        "#plot. It also plot the line y = x through the origin. This line is a visually\n",
        "#representation of the perfect model where all predicted values are equal to the true\n",
        "#values in the test set. The farther the points are from this line, the worse the model\n",
        "#fit.\n",
        "#The reason this plot is called an R2 plot, is because the R2 is simply the squared\n",
        "#correlation between the true and predicted values, which are plotted as paired in the\n",
        "#plot.\n",
        "#In the code below, we use geom_point() and geom_abline() to make this plot using out\n",
        "#advertising_test_results data. The geom_abline() function will plot a line with the\n",
        "#provided slope and intercept arguments.\n",
        "ggplot(data = advertising_test_results,\n",
        "       mapping = aes(x = .pred, y = Sales)) +\n",
        "   geom_point(color = '#006EA1') +\n",
        "   geom_abline(intercept = 0, slope = 1, color = 'orange') +\n",
        "   labs(title = 'Linear Regression Results - Advertising Test Set',\n",
        "        x = 'Predicted Sales',\n",
        "        y = 'Actual Sales')\n",
        "\n",
        "#Creating a Machine Learning Workflow\n",
        "#In the previous steps, we trained a linear regression model to the advertising data\n",
        "#step-by-step. In this section, we will go over how to combine all of the modeling steps\n",
        "#into a single workflow.\n",
        "#We will be using the workflow package, which combines a parnsip model with a recipe,\n",
        "#and the last_fit() function to build an end-to-end modeling training pipeline.\n",
        "\n",
        "#Let's assume we would like to do the following with the advertising data:\n",
        "\n",
        "#1. Split our data into training and test sets\n",
        "#2. Feature engineer the training data by removing skewness and normalizing numeric predictors\n",
        "#3. Specify a linear regression model\n",
        "#4. Train our model on the training data\n",
        "#5. Transform the test data with steps learned in part 2 and obtain predictions using our trained model\n",
        "\n",
        "#The machine learning workflow can be accomplished with a few steps using tidymodels\n",
        "\n",
        "#Step 1. Split Our Data\n",
        "#First we split our data into training and test sets.\n",
        "set.seed(314)\n",
        "\n",
        "# Create a split object\n",
        "advertising_split <- initial_split(advertising, prop = 0.75,\n",
        "                                   strata = Sales)\n",
        "\n",
        "# Build training data set\n",
        "advertising_training <- advertising_split %>%\n",
        "   training()\n",
        "\n",
        "# Build testing data set\n",
        "advertising_test <- advertising_split %>%\n",
        "   testing()\n",
        "\n",
        "#Step 2. Feature Engineering\n",
        "#Here we specify our feature engineering recipe. In this step, we do not use prep() or\n",
        "#bake(). This recipe will be automatically applied in a later step using the workflow()\n",
        "#and last_fit() functions.\n",
        "advertising_recipe <- recipe(Sales ~ ., data = advertising_training) %>%\n",
        "   step_YeoJohnson(all_numeric(), -all_outcomes()) %>%\n",
        "   step_normalize(all_numeric(), -all_outcomes())\n",
        "\n",
        "#Step 3. Specify a Model\n",
        "#Here, we specify our linear regression model with parsnip.\n",
        "lm_model <- linear_reg() %>%\n",
        "   set_engine('lm') %>%\n",
        "   set_mode('regression')\n",
        "\n",
        "#Step 4. Create a Workflow\n",
        "#The workflow package was designed to combine models and recipes into a single object.\n",
        "#create a workflow, we start with workflow() to create an empty workflow and then add\n",
        "#out model and recipe with add_model() and add_recipe().\n",
        "advertising_workflow <- workflow() %>%\n",
        "   add_model(lm_model) %>%\n",
        "   add_recipe(advertising_recipe)\n",
        "\n",
        "#Step 5. Execute the Workflow\n",
        "#The last_fit() function will take a workflow object and apply the recipe and model to\n",
        "#a specified data split object.\n",
        "#In the code below, we pass the advertising_workflow object and advertising_split object\n",
        "#into last_fit().\n",
        "#The last_fit() function will then train the feature engineering steps on the training\n",
        "#data, fit the model to the training data, apply the feature engineering steps to the\n",
        "#test data, and calculate the predictions on the test data, all in one step!\n",
        "advertising_fit <- advertising_workflow %>%\n",
        "   last_fit(split = advertising_split)\n",
        "\n",
        "#To obtain the performance metrics and predictions on the test set, we use the\n",
        "#collect_metrics() and collect_predictions() functions on our advertising_fit object.\n",
        "\n",
        "# Obtain performance metrics on test data\n",
        "advertising_fit %>% collect_metrics()\n",
        "\n",
        "#We can save the test set predictions by using the collect_predictions() function.\n",
        "#This function returns a data frame which will have the response variables values from\n",
        "#the test set and a column named .pred with the model predictions.\n",
        "\n",
        "# Obtain test set predictions data frame\n",
        "test_results <- advertising_fit %>%\n",
        "   collect_predictions()\n",
        "# View results\n",
        "test_results\n",
        "\n",
        "######################################################\n",
        "#-----EXERCISE: Workflow for Home Selling Price------#\n",
        "######################################################\n",
        "\n",
        "#To fit a machine learning workflow, let's use linear regression\n",
        "#to predict the selling price of homes using the home_sales data.\n",
        "#For our feature engineering steps, we will include removing skewness and normalizing\n",
        "#numeric predictors, and creating dummy variables for the \"city\" variable.\n",
        "#Remember that all machine learning algorithms need a numeric feature matrix. Therefore\n",
        "#we must also transform character or factor predictor variables to dummy variables.\n",
        "\n",
        "#In our model formula, we are specifying that selling_price is our response variable\n",
        "#and all others are predictor variables.\n",
        "\n",
        "#Q1: Set seed, split the data into training (75%) and test sets. Call the split object \"homes_split\".\n",
        "#Q2: Build your training data & testing datasets\n",
        "#Q3: Specify your feature engineering recipe. Store this in the object called \"homes_recipe\".\n",
        "#Q4: Check your recipe by prepping it on the training data and applying it to the test data. See if\n",
        "#    you are getting the correct transformations.\n",
        "#Q5: Specify your linear regression model with parsnip.\n",
        "#Q6: Combine your model and recipe o create a workflow object, store it in an object named \"homes_workflow\".\n",
        "#Q7: Execute the Workflow by processing your machine learning workflow with last_fit().\n",
        "#Q8: Obtain the performance metrics and predictions on the test set.\n",
        "#Q9: Save the test set predictions by using the collect_predictions() function.\n",
        "#Q10: Obtain test set predictions data frame and view the results\n",
        "#Q11: Use the homes_results data frame to make an R^2 plot to visualize our\n",
        "#     model performance on the test data set (use ggplot2).\n",
        "#Q12: Explore the variable importance on the training data\n"
      ]
    }
  ]
}